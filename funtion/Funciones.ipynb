{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'divide': 'warn', 'over': 'warn', 'under': 'ignore', 'invalid': 'warn'}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import norm, kurtosis,skew\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import pyplot\n",
    "from statistics import median,mean,stdev\n",
    "from math import sqrt, ceil\n",
    "\n",
    "from numpy import sqrt, newaxis\n",
    "from numpy.fft import irfft, rfftfreq\n",
    "from numpy.random import normal\n",
    "from numpy import sum as npsum\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.mlab as mlab\n",
    "from scipy import stats, optimize\n",
    "\n",
    "np.seterr(under='ignore')\n",
    "np.seterr(over='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grng1(n):\n",
    "    \n",
    "    res = n/12\n",
    "    \n",
    "    data = np.array(pd.DataFrame(np.random.randn(n) * np.sqrt(res) * np.sqrt(1 / n)).cumsum())\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Estadisticos(data):\n",
    "    \n",
    "    nn,mm = np.shape(data)\n",
    "    \n",
    "    Promedio = np.zeros([mm])\n",
    "    Varianza = np.zeros([mm])\n",
    "    Skewness = np.zeros([mm])\n",
    "    Kurtosis = np.zeros([mm])\n",
    "    \n",
    "    for t in range(mm):   \n",
    "        Promedio[t] = np.mean(data[:,t])\n",
    "        Varianza[t] = np.var(data[:,t])\n",
    "        Skewness[t] = skew(data[:,t])\n",
    "        Kurtosis[t] = kurtosis(data[:,t])\n",
    "        \n",
    "    serie_esta = pd.DataFrame({'Prom':Promedio,'Vari':Varianza,'Asim':Skewness,'Curt':Kurtosis})\n",
    "    \n",
    "    return serie_esta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Estadisticos2(data):\n",
    "  \n",
    "    Promedio = np.mean(data)\n",
    "    Varianza = np.var(data)\n",
    "    Skewness = skew(data)\n",
    "    Kurtosis = kurtosis(data)\n",
    "        \n",
    "    serie_esta = pd.DataFrame({'Prom':[Promedio],'Vari':[Varianza],'Asim':[Skewness],'Curt':[Kurtosis]})\n",
    "    \n",
    "    return serie_esta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Normalizar1(data):\n",
    "    \n",
    "    nn,mm = np.shape(data)\n",
    "    \n",
    "    serie_norm = np.zeros([len(data),mm])\n",
    "    Promedio = np.zeros([mm])\n",
    "    Varianza = np.zeros([mm])\n",
    "    Skewness = np.zeros([mm])\n",
    "    Kurtosis = np.zeros([mm])\n",
    "    \n",
    "    for t in range(mm):\n",
    "        for i in range(len(data)):\n",
    "            serie_norm[i,t] = (data[i,t] - np.min(data[:,t]))/(np.max(data[:,t]) - np.min(data[:,t]))\n",
    "    \n",
    "        Promedio[t] = np.mean(serie_norm[:,t])\n",
    "        Varianza[t] = np.var(serie_norm[:,t])\n",
    "        Skewness[t] = skew(serie_norm[:,t])\n",
    "        Kurtosis[t] = kurtosis(serie_norm[:,t])\n",
    "        \n",
    "    if len(data) == 64:\n",
    "        color = 'bo'\n",
    "    elif len(data) == 128:\n",
    "        color = 'ro'\n",
    "    elif len(data) == 256:\n",
    "        color = 'go'\n",
    "    elif len(data) == 512:\n",
    "        color = 'ko'\n",
    "    elif len(data) == 1024:\n",
    "        color = 'co'\n",
    "    elif len(data) == 2048:\n",
    "        color = 'mo'\n",
    "    elif len(data) == 4096:\n",
    "        color = 'yo'\n",
    "    elif len(data) == 8192:\n",
    "        color = 'b*'\n",
    "        \n",
    "    serie_esta = pd.DataFrame({'Group':str(len(data)),'Prom':Promedio,'Vari':Varianza, \\\n",
    "                                'Asim':Skewness,'Curt':Kurtosis,'Cor':color})    \n",
    "        \n",
    "    return serie_norm,serie_esta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Normalizar2(data):\n",
    "    \n",
    "    serie_norm = np.zeros([len(data)])\n",
    "    \n",
    "    for i in range(len(data)):\n",
    "        serie_norm[i] = (data[i] - np.min(data))/(np.max(data) - np.min(data))\n",
    "    \n",
    "    Promedio = np.mean(serie_norm)\n",
    "    Varianza = np.var(serie_norm)\n",
    "    Skewness = skew(serie_norm)\n",
    "    Kurtosis = kurtosis(serie_norm)\n",
    "\n",
    "    serie_esta = pd.DataFrame({'Prom':[Promedio],'Vari':[Varianza],'Asim':[Skewness],'Curt':[Kurtosis]})  \n",
    "        \n",
    "    return serie_norm,serie_esta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pmodel(noValues=1024, p=0.4999, slope=[]):\n",
    "    noOrders = int(np.ceil(np.log2(noValues)))\n",
    "    noValuesGenerated = 2**noOrders\n",
    "    \n",
    "    y = np.array([1])\n",
    "    for n in range(noOrders):\n",
    "        y = next_step_1d(y, p)\n",
    "    \n",
    "    if (slope):\n",
    "        fourierCoeff = fractal_spectrum_1d(noValues, slope/2)\n",
    "        meanVal = np.mean(y)\n",
    "        stdy = np.std(y)\n",
    "        x = np.fft.ifft(y - meanVal)\n",
    "        phase = np.angle(x)\n",
    "        x = fourierCoeff*np.exp(1j*phase)\n",
    "        x = np.fft.fft(x).real\n",
    "        x *= stdy/np.std(x)\n",
    "        x += meanVal\n",
    "    else:\n",
    "        x = y\n",
    "    \n",
    "    return x[0:noValues], y[0:noValues]\n",
    "\n",
    "     \n",
    "def next_step_1d(y, p):\n",
    "    y2 = np.zeros(y.size*2)\n",
    "    sign = np.random.rand(1, y.size) - 0.5\n",
    "    sign /= np.abs(sign)\n",
    "    y2[0:2*y.size:2] = y + sign*(1-2*p)*y\n",
    "    y2[1:2*y.size+1:2] = y - sign*(1-2*p)*y\n",
    "    \n",
    "    return y2\n",
    "\n",
    "\n",
    "def fractal_spectrum_1d(noValues, slope):\n",
    "    ori_vector_size = noValues\n",
    "    ori_half_size = ori_vector_size//2\n",
    "    a = np.zeros(ori_vector_size)\n",
    "    \n",
    "    for t2 in range(ori_half_size):\n",
    "        index = t2\n",
    "        t4 = 1 + ori_vector_size - t2\n",
    "        if (t4 >= ori_vector_size):\n",
    "            t4 = t2\n",
    "        coeff = (index + 1)**slope\n",
    "        a[t2] = coeff\n",
    "        a[t4] = coeff\n",
    "        \n",
    "    a[1] = 0\n",
    "    \n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def powerlaw_psd_gaussian(exponent, size, fmin=0):\n",
    "    \"\"\"Gaussian (1/f)**beta noise.\n",
    "\n",
    "    Based on the algorithm in:\n",
    "    Timmer, J. and Koenig, M.:\n",
    "    On generating power law noise.\n",
    "    Astron. Astrophys. 300, 707-710 (1995)\n",
    "\n",
    "    Normalised to unit variance\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "\n",
    "    exponent : float\n",
    "        The power-spectrum of the generated noise is proportional to\n",
    "\n",
    "        S(f) = (1 / f)**beta\n",
    "        flicker / pink noise:   exponent beta = 1\n",
    "        brown noise:            exponent beta = 2\n",
    "\n",
    "        Furthermore, the autocorrelation decays proportional to lag**-gamma\n",
    "        with gamma = 1 - beta for 0 < beta < 1.\n",
    "        There may be finite-size issues for beta close to one.\n",
    "\n",
    "    shape : int or iterable\n",
    "        The output has the given shape, and the desired power spectrum in\n",
    "        the last coordinate. That is, the last dimension is taken as time,\n",
    "        and all other components are independent.\n",
    "\n",
    "    fmin : float, optional\n",
    "        Low-frequency cutoff.\n",
    "        Default: 0 corresponds to original paper. It is not actually\n",
    "        zero, but 1/samples.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    out : array\n",
    "        The samples.\n",
    "\n",
    "\n",
    "    Examples:\n",
    "    ---------\n",
    "\n",
    "    # generate 1/f noise == pink noise == flicker noise\n",
    "    >>> import colorednoise as cn\n",
    "    >>> y = cn.powerlaw_psd_gaussian(1, 5)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Make sure size is a list so we can iterate it and assign to it.\n",
    "    try:\n",
    "        size = list(size)\n",
    "    except TypeError:\n",
    "        size = [size]\n",
    "    \n",
    "    # The number of samples in each time series\n",
    "    samples = size[-1]\n",
    "    \n",
    "    # Calculate Frequencies (we asume a sample rate of one)\n",
    "    # Use fft functions for real output (-> hermitian spectrum)\n",
    "    f = rfftfreq(samples)\n",
    "    \n",
    "    # Build scaling factors for all frequencies\n",
    "    s_scale = f\n",
    "    fmin = max(fmin, 1./samples) # Low frequency cutoff\n",
    "    ix   = npsum(s_scale < fmin)   # Index of the cutoff\n",
    "    if ix and ix < len(s_scale):\n",
    "        s_scale[:ix] = s_scale[ix]\n",
    "    s_scale = s_scale**(-exponent/2.)\n",
    "    \n",
    "    # Calculate theoretical output standard deviation from scaling\n",
    "    w      = s_scale[1:].copy()\n",
    "    w[-1] *= (1 + (samples % 2)) / 2. # correct f = +-0.5\n",
    "    sigma = 2 * sqrt(npsum(w**2)) / samples\n",
    "    \n",
    "    # Adjust size to generate one Fourier component per frequency\n",
    "    size[-1] = len(f)\n",
    "\n",
    "    # Add empty dimension(s) to broadcast s_scale along last\n",
    "    # dimension of generated random power + phase (below)\n",
    "    dims_to_add = len(size) - 1\n",
    "    s_scale     = s_scale[(newaxis,) * dims_to_add + (Ellipsis,)]\n",
    "    \n",
    "    # Generate scaled random power + phase\n",
    "    sr = normal(scale=s_scale, size=size)\n",
    "    si = normal(scale=s_scale, size=size)\n",
    "    \n",
    "    # If the signal length is even, frequencies +/- 0.5 are equal\n",
    "    # so the coefficient must be real.\n",
    "    if not (samples % 2): si[...,-1] = 0\n",
    "    \n",
    "    # Regardless of signal length, the DC component must be real\n",
    "    si[...,0] = 0\n",
    "    \n",
    "    # Combine power + corrected phase to Fourier components\n",
    "    s  = sr + 1J * si\n",
    "    \n",
    "    # Transform to real time series & scale to unit variance\n",
    "    y = irfft(s, n=samples, axis=-1) / sigma\n",
    "    \n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class cullen_frey():\n",
    "    \"\"\"\n",
    "    Create a skewness-curtosis graph based on Cullen and Frey (1999)\n",
    "    \"\"\"\n",
    "    def __init__(self,data,method='unbiased',discrete=False,boot=None,graph=True):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        data: N x 1 list of sample data\n",
    "        method: 'unbiased' for unbiased estimated values of statistics or 'sample' for sample values.\n",
    "        discrete: If True, the distribution is considered as discrete.\n",
    "        boot: If not None, boot values of skewness and kurtosis are plotted from bootstrap samples of data. \n",
    "        boot must be fixed in this case to an integer above 10.\n",
    "        graph: If False, the skewness-kurtosis graph is not plotted.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.df = data\n",
    "        self.method = method\n",
    "        self.discrete = discrete\n",
    "        self.boot = boot\n",
    "        self.graph = graph\n",
    "        \n",
    "        if not isinstance(self.df,list) or len(np.shape(self.df)) > 1: \n",
    "            raise TypeError('Samples must a list with N x 1 dimensions')\n",
    "        \n",
    "        if len(self.df) < 4:\n",
    "            raise ValueError('The number of samples needs to be greater than 4')\n",
    "            \n",
    "        if self.boot is not None:    \n",
    "            if not isinstance(self.boot,int):\n",
    "                raise ValueError('boot must be integer')\n",
    "            \n",
    "        self.stats()\n",
    "        \n",
    "        \n",
    "    def stats(self):\n",
    "        \"\"\"\n",
    "        Evaluates the min, max, median, mean, skewness and kurtosis of data\n",
    "        \"\"\"\n",
    "        if self.method=='unbiased':        \n",
    "            self.skewdata = skew(self.df, bias=False)\n",
    "            self.kurtdata = kurtosis(self.df, bias=False)+3\n",
    "\n",
    "        elif self.method=='sample':\n",
    "            self.skewdata = skew(self.df, bias=True)\n",
    "            self.kurtdata = kurtosis(self.df, bias=True)+3\n",
    "\n",
    "        \n",
    "        res = [min(self.df),max(self.df),median(self.df),mean(self.df),stdev(self.df),self.skewdata,self.kurtdata]\n",
    "    \n",
    "        #Resumo estatístico\n",
    "        #print(f'min:  {res[0]:.6f}   max: {res[1]:.6f}')\n",
    "        #print(f'median:  {res[2]:.6f}')\n",
    "        #print(f'mean:  {res[3]:.6f}')\n",
    "        #print(f'estimated mean:  {res[4]:.6f}')\n",
    "        #print(f'estimated skewness:  {res[5]:.6f}')\n",
    "        #print(f'estimated kurtosis:  {res[6]:.6f}')\n",
    "\n",
    "        if self.graph:\n",
    "            self.cullen_frey_graph()\n",
    "            \n",
    "    def cullen_frey_graph(self): \n",
    "        \"\"\"\n",
    "        If graph = True, draws the skewness-kurtosis plot\n",
    "        \"\"\"\n",
    "         \n",
    "        if self.boot is not None:\n",
    "            if self.boot < 10:\n",
    "                raise ValueError('boot must be greater than 10')\n",
    "\n",
    "            n = len(self.df)\n",
    "\n",
    "            nrow = n\n",
    "            ncol = self.boot\n",
    "            databoot = np.reshape(np.random.choice(self.df, size=n*self.boot, replace=True),(nrow,ncol)) \n",
    "\n",
    "            s2boot = (skew(pd.DataFrame(databoot)))**2\n",
    "            kurtboot = kurtosis(pd.DataFrame(databoot))+3\n",
    "\n",
    "            kurtmax = max(10,ceil(max(kurtboot)))\n",
    "            xmax = max(4,ceil(max(s2boot)))\n",
    "\n",
    "        else:\n",
    "            kurtmax = max(10,ceil(self.kurtdata))\n",
    "            xmax = max(4,ceil(self.skewdata**2))\n",
    "\n",
    "        ymax = kurtmax-1\n",
    "        \n",
    "        # If discrete = False\n",
    "        if not self.discrete:\n",
    "            #Beta distribution\n",
    "            p = np.exp(-100)\n",
    "            lq = np.arange(-100,100.1,0.1)\n",
    "            q = np.exp(lq)\n",
    "            s2a = (4*(q-p)**2*(p+q+1))/((p+q+2)**2*p*q)\n",
    "            ya = kurtmax-(3*(p+q+1)*(p*q*(p+q-6)+2*(p+q)**2)/(p*q*(p+q+2)*(p+q+3)))\n",
    "            p = np.exp(100)\n",
    "            lq = np.arange(-100,100.1,0.1)\n",
    "            q = np.exp(lq)\n",
    "            s2b = (4*(q-p)**2*(p+q+1))/((p+q+2)**2*p*q)\n",
    "            yb = kurtmax-(3*(p+q+1)*(p*q*(p+q-6)+2*(p+q)**2)/(p*q*(p+q+2)*(p+q+3)))\n",
    "            s2 = [*s2a,*s2b]\n",
    "            y = [*ya,*yb]\n",
    "\n",
    "            #Gama distribution\n",
    "            lshape_gama = np.arange(-100,100,0.1)\n",
    "            shape_gama = np.exp(lshape_gama)\n",
    "            s2_gama = 4/shape_gama\n",
    "            y_gama = kurtmax-(3+6/shape_gama) \n",
    "\n",
    "            #Lognormal distribution\n",
    "            lshape_lnorm = np.arange(-100,100,0.1)\n",
    "            shape_lnorm = np.exp(lshape_lnorm)\n",
    "            es2_lnorm = np.exp(shape_lnorm**2, dtype=np.float64)\n",
    "            s2_lnorm = (es2_lnorm+2)**2*(es2_lnorm-1)\n",
    "            y_lnorm = kurtmax-(es2_lnorm**4+2*es2_lnorm**3+3*es2_lnorm**2-3)\n",
    "\n",
    "            plt.figure(figsize=(12,9))\n",
    "            \n",
    "            #observations\n",
    "            obs = plt.scatter(self.skewdata**2,kurtmax-self.kurtdata,s=200, c='blue', \n",
    "                              label='Observation',zorder=10)\n",
    "            #beta\n",
    "            beta = plt.fill(s2,y,color='lightgrey',alpha=0.6, label='beta', zorder=0)\n",
    "            #gama\n",
    "            gama = plt.plot(s2_gama,y_gama, '--', c='k', label='gama')\n",
    "            #lognormal\n",
    "            lnormal = plt.plot(s2_lnorm,y_lnorm, c='k', label='lognormal')\n",
    "            \n",
    "            if self.boot is not None:\n",
    "                #bootstrap \n",
    "                bootstrap = plt.scatter(s2boot,kurtmax-kurtboot,marker='$\\circ$',c='orange',s=50, \n",
    "                                        label='Bootstrap values', zorder=5)\n",
    "                legenda1 = plt.legend(handles=[bootstrap],loc=(xmax*0.1065,ymax*0.101), \n",
    "                                      labelspacing=2, frameon=False)\n",
    "                plt.gca().add_artist(legenda1)\n",
    "            \n",
    "\n",
    "            #markers\n",
    "            normal = plt.scatter(0,kurtmax-3, marker=(8,2,0),s=200,c='k',label='normal',zorder=5)\n",
    "\n",
    "            uniform = plt.scatter(0,kurtmax-9/5, marker='$\\\\bigtriangleup$',s=200,c='k',\\\n",
    "                                  label='uniform',zorder=5)   \n",
    "\n",
    "            exp_dist = plt.scatter(2**2,kurtmax-9, marker='$\\\\bigotimes$',s=200,c='k',\\\n",
    "                                   label='exponential',zorder=5) \n",
    "\n",
    "            logistic = plt.scatter(0,kurtmax-4.2, marker='+',s=200,c='k',label='logistic',zorder=5)\n",
    "\n",
    "\n",
    "            #Adjusting the axis\n",
    "            yax = [str(kurtmax - i) for i in range(0,ymax+1)]\n",
    "            plt.xlim(-1, xmax+0.4)\n",
    "            plt.ylim(-1, ymax+0.08)\n",
    "            plt.yticks(np.arange(0,ymax+2,5),labels=yax,fontsize=14)\n",
    "            plt.xticks(fontsize=14)\n",
    "\n",
    "            #Adding the labels\n",
    "            plt.xlabel('square of skewness', fontsize=18)\n",
    "            plt.ylabel('kurtosis', fontsize=18)\n",
    "            plt.title('Cullen and Frey graph', fontsize=20) \n",
    "\n",
    "            #Adding the legends\n",
    "            legenda2 = plt.legend(handles=[obs],loc='upper center', labelspacing=1, frameon=False)\n",
    "            plt.gca().add_artist(legenda2)\n",
    "\n",
    "            plt.legend(handles=[normal,uniform,exp_dist,logistic,beta[0],lnormal[0],gama[0]], \n",
    "                       title='Theoretical distributions',loc='lower left',labelspacing=1.4,frameon=False,\n",
    "                       prop={'size': 16})\n",
    "\n",
    "            plt.show()\n",
    "        \n",
    "        #If discrete = True\n",
    "        else:\n",
    "            # negbin distribution\n",
    "            p = np.exp(-10)\n",
    "            lr = np.arange(-100,100,0.1)\n",
    "            r = np.exp(lr)\n",
    "            s2a = (2-p)**2/(r*(1-p))\n",
    "            ya = kurtmax-(3+6/r+p**2/(r*(1-p)))\n",
    "            p = 1-np.exp(-10)\n",
    "            lr = np.arange(100,-100,-0.1)\n",
    "            r = np.exp(lr)\n",
    "            s2b = (2-p)**2/(r*(1-p))\n",
    "            yb = kurtmax-(3+6/r+p**2/(r*(1-p)))\n",
    "            s2_negbin = [*s2a,*s2b]\n",
    "            y_negbin = [*ya,*yb]\n",
    "            \n",
    "            # poisson distribution\n",
    "            llambda = np.arange(-100,100,0.1)\n",
    "            lambda_ = np.exp(llambda)\n",
    "            s2_poisson = 1/lambda_\n",
    "            y_poisson = kurtmax-(3+1/lambda_)\n",
    "            \n",
    "            plt.figure(figsize=(12,9))          \n",
    "            \n",
    "            #observations\n",
    "            obs = plt.scatter(self.skewdata**2,kurtmax-self.kurtdata,s=200, c='blue', \n",
    "                              label='Observation',zorder=10)\n",
    "            \n",
    "            #negative binomial\n",
    "            negbin = plt.fill(s2_negbin,y_negbin,color='lightgrey',alpha=0.6, \\\n",
    "                              label='negative binomial', zorder=0)\n",
    "            \n",
    "            #poisson\n",
    "            poisson = plt.plot(s2_poisson,y_poisson, '--', c='k', label='poisson')\n",
    "            \n",
    "            if self.boot is not None:\n",
    "                #bootstrap \n",
    "                bootstrap = plt.scatter(s2boot,kurtmax-kurtboot,marker='$\\circ$',c='orange',s=50, \n",
    "                                        label='Bootstrap values', zorder=5)\n",
    "                legenda2 = plt.legend(handles=[bootstrap],loc=(xmax*0.1065,ymax*0.101), \n",
    "                                      labelspacing=2, frameon=False)\n",
    "                plt.gca().add_artist(legenda2)\n",
    "            \n",
    "            #markers\n",
    "            normal = plt.scatter(0,kurtmax-3, marker=(8,2,0),s=400,c='k',label='normal',zorder=5)\n",
    "            \n",
    "            #adjusting the axis\n",
    "            yax = [str(kurtmax - i) for i in range(0,ymax+1)]\n",
    "            plt.xlim(-1, xmax+0.4)\n",
    "            plt.ylim(-1, ymax+1)\n",
    "            \n",
    "            plt.yticks(np.arange(0,ymax+2,4),labels=yax)\n",
    "            #plt.yticks(list(range(0,ymax+1,2)),labels=yax)\n",
    "\n",
    "            #adding the labels\n",
    "            plt.xlabel('square of skewness', fontsize=18)\n",
    "            plt.ylabel('kurtosis', fontsize=18)\n",
    "            plt.title('Cullen and Frey graph', fontsize=20) \n",
    "            \n",
    "            #adding the legends\n",
    "            legenda1 = plt.legend(handles=[obs],loc='upper center', labelspacing=1, frameon=False)\n",
    "            plt.gca().add_artist(legenda1)\n",
    "\n",
    "            plt.legend(handles=[normal,negbin[0],poisson[0]],title='Theoretical distributions',\\\n",
    "                       loc='upper right',labelspacing=1.4,frameon=False)\n",
    "            \n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gerador de Mapa Logístico Caótico 1D: Atrator e Série Temporal\n",
    "#1D Chaotic Logistic Map Generator: Attractor and Time Series\n",
    "#Reinaldo R. Rosa - LABAC-INPE\n",
    "#Version 1.0 for CAP239-2020\n",
    "\n",
    "#chaotic logistic map is f(x) = rho*x*(1-x)  with rho in (3.81,4.00)\n",
    "def Logistic(rho,tau,x,y):\n",
    "\n",
    "    return rho*x*(1.0-x), tau*x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gerador de Mapa Logístico Caótico 2D (Henon Map): Atrator e Série Temporal\n",
    "#2D Chaotic Logistic Map Generator (Henon Map): Attractor and Time Series\n",
    "#Reinaldo R. Rosa - LABAC-INPE\n",
    "#Version 1.0 for CAP239-2020\n",
    "\n",
    "#2D Henon logistic map is noise-like with \"a\" in (1.350,1.420) and \"b\" in (0.210,0.310)\n",
    "def HenonMap(a,b,x,y):\n",
    "\n",
    "    return y + 1.0 - a *x*x, b * x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def psd(data):\n",
    "\n",
    "    \"\"\"Calcula o PSD de uma série temporal.\"\"\"\n",
    "\n",
    "    # Define um intervalo para realizar o ajuste da reta\n",
    "\n",
    "    INICIO = 10\n",
    "\n",
    "    FIM = 200\n",
    "\n",
    "    # O vetor com o tempo é o tamanho do número de pontos\n",
    "\n",
    "    N = len(data)\n",
    "\n",
    "    tempo = np.arange(len(data))\n",
    "\n",
    "    # Define a frequência de amostragem\n",
    "\n",
    "    dt = (tempo[-1] - tempo[0] / (N - 1))\n",
    "\n",
    "    fs = 1 / dt\n",
    "\n",
    "    # Calcula o PSD utilizando o MLAB\n",
    "\n",
    "    power, freqs = mlab.psd(data, Fs = fs, NFFT = N, scale_by_freq = False)\n",
    "\n",
    "    # Calcula a porcentagem de pontos utilizados na reta de ajuste\n",
    "\n",
    "    totalFrequencias = len(freqs)\n",
    "\n",
    "    totalPSD = FIM - INICIO\n",
    "\n",
    "    porcentagemPSD = int(100 * totalPSD / totalFrequencias)\n",
    "\n",
    "    # Seleciona os dados dentro do intervalo de seleção\n",
    "\n",
    "    xdata = freqs[INICIO:FIM]\n",
    "\n",
    "    ydata = power[INICIO:FIM]\n",
    "\n",
    "    # Simula o erro\n",
    "\n",
    "    yerr = 0.2 * ydata\n",
    "\n",
    "    # Define uma função para calcular a Lei de Potência\n",
    "\n",
    "    powerlaw = lambda x, amp, index: amp * (x**index)\n",
    "\n",
    "    # Converte os dados para o formato LOG\n",
    "\n",
    "    logx = np.log10(xdata)\n",
    "\n",
    "    logy = np.log10(ydata)\n",
    "\n",
    "    # Define a função para realizar o ajuste\n",
    "\n",
    "    fitfunc = lambda p, x: p[0] + p[1] * x\n",
    "\n",
    "    errfunc = lambda p, x, y, err: (y - fitfunc(p, x)) / err    \n",
    "\n",
    "    logyerr = yerr / ydata\n",
    "\n",
    "    # Calcula a reta de ajuste\n",
    "\n",
    "    pinit = [1.0, -1.0]\n",
    "\n",
    "    out = optimize.leastsq(errfunc, pinit, args = (logx, logy, logyerr), full_output = 1)    \n",
    "\n",
    "    pfinal = out[0]\n",
    "\n",
    "    covar = out[1]\n",
    "\n",
    "    index = -1*pfinal[1]\n",
    "\n",
    "    amp = 10.0 ** pfinal[0]\n",
    "\n",
    "    indexErr = np.sqrt(covar[0][0])\n",
    "\n",
    "    ampErr = np.sqrt(covar[1][1]) * amp\n",
    "\n",
    "    # Retorna os valores obtidos\n",
    "\n",
    "    return freqs, power, xdata, ydata, amp, index, powerlaw, INICIO, FIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def psd2(data):\n",
    "\n",
    "    \"\"\"Calcula o PSD de uma série temporal.\"\"\"\n",
    "\n",
    "    # Define um intervalo para realizar o ajuste da reta\n",
    "\n",
    "    INICIO = 10\n",
    "\n",
    "    FIM = 200\n",
    "\n",
    "    # O vetor com o tempo é o tamanho do número de pontos\n",
    "\n",
    "    N = len(data)\n",
    "\n",
    "    tempo = np.arange(len(data))\n",
    "\n",
    "    # Define a frequência de amostragem\n",
    "\n",
    "    dt = (tempo[-1] - tempo[0] / (N - 1))\n",
    "\n",
    "    fs = 1 / dt\n",
    "\n",
    "    # Calcula o PSD utilizando o MLAB\n",
    "\n",
    "    power, freqs = mlab.psd(data, Fs = fs, NFFT = N, scale_by_freq = False)\n",
    "\n",
    "    # Calcula a porcentagem de pontos utilizados na reta de ajuste\n",
    "\n",
    "    totalFrequencias = len(freqs)\n",
    "\n",
    "    totalPSD = FIM - INICIO\n",
    "\n",
    "    porcentagemPSD = int(100 * totalPSD / totalFrequencias)\n",
    "\n",
    "    # Seleciona os dados dentro do intervalo de seleção\n",
    "\n",
    "    xdata = freqs[INICIO:FIM]\n",
    "\n",
    "    ydata = power[INICIO:FIM]\n",
    "\n",
    "    # Simula o erro\n",
    "\n",
    "    yerr = 0.2 * ydata\n",
    "\n",
    "    # Define uma função para calcular a Lei de Potência\n",
    "\n",
    "    powerlaw = lambda x, amp, index: amp * (x**index)\n",
    "\n",
    "    # Converte os dados para o formato LOG\n",
    "\n",
    "    logx = np.log10(xdata)\n",
    "\n",
    "    logy = np.log10(ydata)\n",
    "\n",
    "    # Define a função para realizar o ajuste\n",
    "\n",
    "    fitfunc = lambda p, x: p[0] + p[1] * x\n",
    "\n",
    "    errfunc = lambda p, x, y, err: (y - fitfunc(p, x)) / err    \n",
    "\n",
    "    logyerr = yerr / ydata\n",
    "\n",
    "    # Calcula a reta de ajuste\n",
    "\n",
    "    pinit = [1.0, -1.0]\n",
    "\n",
    "    out = optimize.leastsq(errfunc, pinit, args = (logx, logy, logyerr), full_output = 1)    \n",
    "\n",
    "    pfinal = out[0]\n",
    "\n",
    "    covar = out[1]\n",
    "\n",
    "    index = -1*pfinal[1]\n",
    "\n",
    "    amp = 10.0 ** pfinal[0]\n",
    "\n",
    "    indexErr = np.sqrt(covar[0][0])\n",
    "\n",
    "    ampErr = np.sqrt(covar[1][1]) * amp\n",
    "\n",
    "    # Retorna os valores obtidos\n",
    "\n",
    "    return freqs, power, xdata, ydata, amp, index, powerlaw, INICIO, FIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dfa1d(timeSeries, grau):\n",
    "\n",
    "    \"\"\"Calcula o DFA 1D (adaptado de Physionet), onde a escala cresce\n",
    "\n",
    "    de acordo com a variável 'Boxratio'. Retorna o array 'vetoutput', \n",
    "\n",
    "    onde a primeira coluna é o log da escala S e a segunda coluna é o\n",
    "\n",
    "    log da função de flutuação.\"\"\"\n",
    "\n",
    "\n",
    "    # 1. A série temporal {Xk} com k = 1, ..., N é integrada na chamada função perfil Y(k)\n",
    "\n",
    "    x = np.mean(timeSeries)\n",
    "\n",
    "    timeSeries = timeSeries - x\n",
    "\n",
    "    yk = np.cumsum(timeSeries)\n",
    "\n",
    "    tam = len(timeSeries)\n",
    "\n",
    "\n",
    "    # 2. A série (ou perfil) Y(k) é dividida em N intervalos não sobrepostos de tamanho S\n",
    "\n",
    "    sf = np.ceil(tam / 4).astype(np.int)\n",
    "\n",
    "    boxratio = np.power(2.0, 1.0 / 8.0)\n",
    "\n",
    "    vetoutput = np.zeros(shape = (1,2))\n",
    "\n",
    "\n",
    "    s = 4\n",
    "\n",
    "    while s <= sf:        \n",
    "\n",
    "        serie = yk        \n",
    "\n",
    "        if np.mod(tam, s) != 0:\n",
    "\n",
    "            l = s * int(np.trunc(tam/s))\n",
    "\n",
    "            serie = yk[0:l]\t\t\t\n",
    "\n",
    "        t = np.arange(s, len(serie), s)\n",
    "\n",
    "        v = np.array(np.array_split(serie, t))\n",
    "\n",
    "        l = len(v)\n",
    "\n",
    "        x = np.arange(1, s + 1)\n",
    "\n",
    "\n",
    "        # 3. Calcula-se a variância para cada segmento v = 1,…, n_s:\n",
    "\n",
    "        p = np.polynomial.polynomial.polyfit(x, v.T, grau)\n",
    "\n",
    "        yfit = np.polynomial.polynomial.polyval(x, p)\n",
    "\n",
    "        vetvar = np.var(v - yfit)\n",
    "\n",
    "\n",
    "    # 4. Calcula-se a função de flutuação DFA como a média das variâncias de cada intervalo\n",
    "\n",
    "        fs = np.sqrt(np.mean(vetvar))\n",
    "\n",
    "        vetoutput = np.vstack((vetoutput,[s, fs]))\n",
    "\n",
    "\n",
    "        # A escala S cresce numa série geométrica\n",
    "\n",
    "        s = np.ceil(s * boxratio).astype(np.int)\n",
    "\n",
    "\n",
    "    # Array com o log da escala S e o log da função de flutuação   \n",
    "\n",
    "    vetoutput = np.log10(vetoutput[1::1,:])\n",
    "\n",
    "\n",
    "    # Separa as colunas do vetor 'vetoutput'\n",
    "\n",
    "    x = vetoutput[:,0]\n",
    "\n",
    "    y = vetoutput[:,1]\n",
    "\n",
    "\n",
    "    # Regressão linear\n",
    "\n",
    "    slope, intercept, _, _, _ = stats.linregress(x, y)\n",
    "\n",
    "\n",
    "    # Calcula a reta de inclinação\n",
    "\n",
    "    predict_y = intercept + slope * x\n",
    "\n",
    "\n",
    "    # Calcula o erro\n",
    "\n",
    "    pred_error = y - predict_y\n",
    "\n",
    "\n",
    "    # Retorna o valor do ALFA, o vetor 'vetoutput', os vetores X e Y,\n",
    "\n",
    "    # o vetor com os valores da reta de inclinação e o vetor de erros\n",
    "\n",
    "    return slope, vetoutput, x, y, predict_y, pred_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getHurstByUpscaling(dx, normType_p = np.inf, isDFA = 1, normType_q = 1.0):\n",
    "    ## Some initialiation\n",
    "    dx_len = len(dx)\n",
    "    \n",
    "    # We have to reserve the most major scale for shifts, so we divide the data\n",
    "    # length by two. (As a result, the time measure starts from 2.0, not from\n",
    "    # 1.0, see below.)\n",
    "    dx_len = np.int(dx_len / 2)\n",
    "    \n",
    "    dx_shift = np.int(dx_len / 2)\n",
    "    \n",
    "    nScales = np.int(np.round(np.log2(dx_len)))    # Number of scales involved. P.S. We use 'round()' to prevent possible malcomputing of the logarithms\n",
    "    j = 2 ** (np.arange(1, nScales + 1) - 1) - 1\n",
    "    \n",
    "    meanDataMeasure = np.zeros(nScales)\n",
    "    \n",
    "    ## Computing the data measure\n",
    "    for ji in range(1, nScales + 1):\n",
    "        # At the scale 'j(ji)' we deal with '2 * (j(ji) + 1)' elements of the data 'dx'\n",
    "        dx_k_len = 2 * (j[ji - 1] + 1)\n",
    "        n = np.int(dx_len / dx_k_len)\n",
    "        \n",
    "        dx_leftShift = np.int(dx_k_len / 2)\n",
    "        dx_rightShift = np.int(dx_k_len / 2)\n",
    "        \n",
    "        for k in range(1, n + 1):\n",
    "            # We get a portion of the data of the length '2 * (j(ji) + 1)' plus the data from the left and right boundaries\n",
    "            dx_k_withShifts = dx[(k - 1) * dx_k_len + 1 + dx_shift - dx_leftShift - 1 : k * dx_k_len + dx_shift + dx_rightShift]\n",
    "            \n",
    "            # Then we perform free upscaling and, using the above-selected data (provided at the scale j = 0),\n",
    "            # compute the velocities at the scale 'j(ji)'\n",
    "            j_dx = np.convolve(dx_k_withShifts, np.ones(dx_rightShift), 'valid')\n",
    "            \n",
    "            # Then we compute the accelerations at the scale 'j(ji) + 1'\n",
    "            r = (j_dx[1 + dx_rightShift - 1 : ] - j_dx[1 - 1 : -dx_rightShift]) / 2.0\n",
    "            \n",
    "            # Finally, we compute the range ...\n",
    "            if (normType_p == 0):\n",
    "                R = np.max(r[2 - 1 : ]) - np.min(r[2 - 1 : ])\n",
    "            elif (np.isinf(normType_p)):\n",
    "                R = np.max(np.abs(r[2 - 1 : ]))\n",
    "            else:\n",
    "                R = (np.sum(r[2 - 1 : ] ** normType_p) / len(r[2 - 1 : ])) ** (1.0 / normType_p)\n",
    "            # ... and the normalisation factor (\"standard deviation\")\n",
    "            S = np.sqrt(np.sum(np.abs(np.diff(r)) ** 2.0) / (len(r) - 1))\n",
    "            if (isDFA == 1):\n",
    "                S = 1.0\n",
    "            \n",
    "            meanDataMeasure[ji - 1] += (R / S) ** normType_q\n",
    "        meanDataMeasure[ji - 1] = (meanDataMeasure[ji - 1] / n) ** (1.0 / normType_q)\n",
    "    \n",
    "    # We pass from the scales ('j') to the time measure; the time measure at the scale j(nScales) (the most major one)\n",
    "    # is assumed to be 2.0, while it is growing when the scale is tending to j(1) (the most minor one).\n",
    "    # (The scale j(nScales)'s time measure is NOT equal to 1.0, because we reserved the highest scale for shifts\n",
    "    # in the very beginning of the function.)\n",
    "    timeMeasure = 2.0 * dx_len / (2 * (j + 1))\n",
    "    \n",
    "    scales = j + 1\n",
    "    \n",
    "    return [timeMeasure, meanDataMeasure, scales]\n",
    "\n",
    "#MFDFA-Analytics-by-SKDataScience\n",
    "#multifractal DFA singularity spectra - module 02\n",
    "#Version 3.0 - Modified by R.R.Rosa - Dec 2018 - mfdfa_ss_m2.py\n",
    "# This code implements a modification of the first-order multifractal analysis algorithm. It is based on the\n",
    "# corresponding unifractal analysis technique described in [1]. It computes the Lipschitz-Holder multifractal\n",
    "# singularity spectrum, as well as the minimum and maximum generalised Hurst exponents [2, 3].\n",
    "#\n",
    "# At the input, 'dx' is a time series of increments of the physical observable 'x(t)', of the length equal to an\n",
    "# integer power of two greater than two (i.e. 4, 8, 16, 32, etc.), 'normType' is any real greater than or\n",
    "# equal to one specifying the p-norm, 'isDFA' is a boolean value prescribing to use either the DFA-based algorithm or\n",
    "# the standard Hurst (a.k.a. R/S) analysis, 'isNormalised' is a boolean value prescribing either to normalise the\n",
    "# intermediate range-to-deviation (R/S) expression or to proceed computing without normalisation.\n",
    "#\n",
    "# At the output, 'timeMeasure' is the time measure of the data's support at different scales, 'dataMeasure' is\n",
    "# the data measure at different scales computed for each value of the variable q-norm, 'scales' is the scales at which\n",
    "# the data measure is computed, 'stats' is the structure containing MF-DFA statistics, while 'q' is the values of the\n",
    "# q-norm used.\n",
    "#\n",
    "# Similarly to unifractal analysis (see getHurstByUpscaling()), the time measure is computed merely for an alternative\n",
    "# representation of the dependence 'dataMeasure(q, scales) ~ scales ^ -tau(q)'.\n",
    "\n",
    "def getMSSByUpscaling(dx, normType = np.inf, isDFA = 1, isNormalised = 1):\n",
    "    ## Some initialiation\n",
    "    aux_eps = np.finfo(float).eps\n",
    "    \n",
    "    # We prepare an array of values of the variable q-norm\n",
    "    aux = [-16.0, -8.0, -4.0, -2.0, -1.0, -0.5, -0.0001, 0.0, 0.0001, 0.5, 0.9999, 1.0, 1.0001, 2.0, 4.0, 8.0, 16.0, 32.0]\n",
    "    nq = len(aux)\n",
    "    \n",
    "    q = np.zeros((nq, 1))\n",
    "    q[:, 1 - 1] = aux\n",
    "    \n",
    "    dx_len = len(dx)\n",
    "    \n",
    "    # We have to reserve the most major scale for shifts, so we divide the data\n",
    "    # length by two. (As a result, the time measure starts from 2.0, not from\n",
    "    # 1.0, see below.)\n",
    "    dx_len = np.int(dx_len / 2)\n",
    "    \n",
    "    dx_shift = np.int(dx_len / 2)\n",
    "    \n",
    "    nScales = np.int(np.round(np.log2(dx_len)))    # Number of scales involved. P.S. We use 'round()' to prevent possible malcomputing of the logarithms\n",
    "    j = 2 ** (np.arange(1, nScales + 1) - 1) - 1\n",
    "    \n",
    "    dataMeasure = np.zeros((nq, nScales))\n",
    "    \n",
    "    ## Computing the data measures in different q-norms\n",
    "    for ji in range(1, nScales + 1):\n",
    "        # At the scale 'j(ji)' we deal with '2 * (j(ji) + 1)' elements of the data 'dx'\n",
    "        dx_k_len = 2 * (j[ji - 1] + 1)\n",
    "        n = np.int(dx_len / dx_k_len)\n",
    "        \n",
    "        dx_leftShift = np.int(dx_k_len / 2)\n",
    "        dx_rightShift = np.int(dx_k_len / 2)\n",
    "        \n",
    "        R = np.zeros(n)\n",
    "        S = np.ones(n)\n",
    "        for k in range(1, n + 1):\n",
    "            # We get a portion of the data of the length '2 * (j(ji) + 1)' plus the data from the left and right boundaries\n",
    "            dx_k_withShifts = dx[(k - 1) * dx_k_len + 1 + dx_shift - dx_leftShift - 1 : k * dx_k_len + dx_shift + dx_rightShift]\n",
    "            \n",
    "            # Then we perform free upscaling and, using the above-selected data (provided at the scale j = 0),\n",
    "            # compute the velocities at the scale 'j(ji)'\n",
    "            j_dx = np.convolve(dx_k_withShifts, np.ones(dx_rightShift), 'valid')\n",
    "            \n",
    "            # Then we compute the accelerations at the scale 'j(ji) + 1'\n",
    "            r = (j_dx[1 + dx_rightShift - 1 : ] - j_dx[1 - 1 : -dx_rightShift]) / 2.0\n",
    "            \n",
    "            # Finally we compute the range ...\n",
    "            if (normType == 0):\n",
    "                R[k - 1] = np.max(r[2 - 1 : ]) - np.min(r[2 - 1 : ])\n",
    "            elif (np.isinf(normType)):\n",
    "                R[k - 1] = np.max(np.abs(r[2 - 1 : ]))\n",
    "            else:\n",
    "                R[k - 1] = (np.sum(r[2 - 1 : ] ** normType) / len(r[2 - 1 : ])) ** (1.0 / normType)\n",
    "            # ... and the normalisation factor (\"standard deviation\")\n",
    "            if (isDFA == 0):\n",
    "                S[k - 1] = np.sqrt(np.sum(np.abs(np.diff(r)) ** 2.0) / (len(r) - 1))\n",
    "    \n",
    "        if (isNormalised == 1):      # Then we either normalise the R / S values, treating them as probabilities ...\n",
    "            p = np.divide(R, S) / np.sum(np.divide(R, S))\n",
    "        else:                        # ... or leave them unnormalised ...\n",
    "            p = np.divide(R, S)\n",
    "          # ... and compute the measures in the q-norms\n",
    "        for k in range(1, n + 1):\n",
    "            # This 'if' is needed to prevent measure blow-ups with negative values of 'q' when the probability is close to zero\n",
    "            if (p[k - 1] < 1000.0 * aux_eps):\n",
    "                continue\n",
    "            \n",
    "            dataMeasure[:, ji - 1] = dataMeasure[:, ji - 1] + np.power(p[k - 1], q[:, 1 - 1])\n",
    "\n",
    "    # We pass from the scales ('j') to the time measure; the time measure at the scale j(nScales) (the most major one)\n",
    "    # is assumed to be 2.0, while it is growing when the scale is tending to j(1) (the most minor one).\n",
    "    # (The scale j(nScales)'s time measure is NOT equal to 1.0, because we reserved the highest scale for shifts\n",
    "    # in the very beginning of the function.)\n",
    "    timeMeasure = 2.0 * dx_len / (2 * (j + 1))\n",
    "    \n",
    "    scales = j + 1\n",
    "    \n",
    "    ## Determining the exponents 'tau' from 'dataMeasure(q, timeMeasure) ~ timeMeasure ^ tau(q)'\n",
    "    tau = np.zeros((nq, 1))\n",
    "    log10tm = np.log10(timeMeasure)\n",
    "    log10dm = np.log10(dataMeasure)\n",
    "    log10tm_mean = np.mean(log10tm)\n",
    "    \n",
    "    # For each value of the q-norm we compute the mean 'tau' over all the scales\n",
    "    for qi in range(1, nq + 1):\n",
    "        tau[qi - 1, 1 - 1] = np.sum(np.multiply(log10tm, (log10dm[qi - 1, :] - np.mean(log10dm[qi - 1, :])))) / np.sum(np.multiply(log10tm, (log10tm - log10tm_mean)))\n",
    "\n",
    "    ## Finally, we only have to pass from 'tau(q)' to its conjugate function 'f(alpha)'\n",
    "    # In doing so, first we find the Lipschitz-Holder exponents 'alpha' (represented by the variable 'LH') ...\n",
    "    aux_top = (tau[2 - 1] - tau[1 - 1]) / (q[2 - 1] - q[1 - 1])\n",
    "    aux_middle = np.divide(tau[3 - 1 : , 1 - 1] - tau[1 - 1 : -1 - 1, 1 - 1], q[3 - 1 : , 1 - 1] - q[1 - 1 : -1 - 1, 1 - 1])\n",
    "    aux_bottom = (tau[-1] - tau[-1 - 1]) / (q[-1] - q[-1 - 1])\n",
    "    LH = np.zeros((nq, 1))\n",
    "    LH[:, 1 - 1] = -np.concatenate((aux_top, aux_middle, aux_bottom))\n",
    "    # ... and then compute the conjugate function 'f(alpha)' itself\n",
    "    f = np.multiply(LH, q) + tau\n",
    "\n",
    "    ## The last preparations\n",
    "    # We determine the minimum and maximum values of 'alpha' ...\n",
    "    LH_min = LH[-1, 1 - 1]\n",
    "    LH_max = LH[1 - 1, 1 - 1]\n",
    "    # ... and find the minimum and maximum values of another multifractal characteristic, the so-called\n",
    "    # generalised Hurst (or DFA) exponent 'h'. (These parameters are computed according to [2, p. 27].)\n",
    "    h_min = -(1.0 + tau[-1, 1 - 1]) / q[-1, 1 - 1]\n",
    "    h_max = -(1.0 + tau[1 - 1, 1 - 1]) / q[1 - 1, 1 - 1]\n",
    "    \n",
    "    stats = {'tau':       tau,\n",
    "        'LH':        LH,\n",
    "            'f':         f,\n",
    "                'LH_min':    LH_min,\n",
    "                    'LH_max':    LH_max,\n",
    "                        'h_min':     h_min,\n",
    "                            'h_max':     h_max}\n",
    "    \n",
    "    return [timeMeasure, dataMeasure, scales, stats, q]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################################################\n",
    "# Compute and plot Detrended Fluctuation Analysis\n",
    "#\n",
    "# Adapted from:\n",
    "# https://github.com/reinaldo-rosa-inpe/cap239/blob/f92f84710bb8438e461d2bf5d237e93bb7238d3e/Codigos/Specplus.py\n",
    "# Written by Paulo Giovani, Reinaldo Roberto Rosa, Murilo da Silva Dantas\n",
    "#\n",
    "# Adapted by Rian Koja to publish in a GitHub repository with GPL licence for this specific file.\n",
    "########################################################################################################################\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "# Specplus.pyplot\n",
    "# ------------------------------------------------------------------------\n",
    "# ---------------------------------------------------------------------\n",
    "# Computes PSD of a time series\n",
    "# Optionally receives an interval for the linear regression step\n",
    "# ---------------------------------------------------------------------\n",
    "def psd(data, init=0, final=None):\n",
    "    n = len(data)\n",
    "    time = np.arange(n)\n",
    "\n",
    "    # If \"final\" is not given, use length of data\n",
    "    if final is None:\n",
    "        final = n\n",
    "\n",
    "    # Define sampling frequency:\n",
    "    dt = (time[-1] - time[0] / (n - 1))\n",
    "    fs = 1 / dt\n",
    "\n",
    "    # Compute PSD with MLAB:\n",
    "    power, freqs = mlab.psd(data, Fs=fs, NFFT=n, scale_by_freq=False)\n",
    "\n",
    "    # Select data within selction interval\n",
    "    xdata = freqs[init:final]\n",
    "    ydata = power[init:final]\n",
    "\n",
    "    # Simulate error\n",
    "    yerr = 0.2 * ydata\n",
    "\n",
    "    # Find logarithms of data:\n",
    "    logx = np.log10(xdata)\n",
    "    logy = np.log10(ydata)\n",
    "\n",
    "    logyerr = yerr / ydata\n",
    "\n",
    "    # Compute line fit:\n",
    "    pinit = [1.0, -1.0]\n",
    "    out = optimize.leastsq(errfunc, pinit, args=(logx, logy, logyerr), full_output=True)\n",
    "    pfinal = out[0]\n",
    "    index = pfinal[1]\n",
    "    amp = 10.0 ** pfinal[0]\n",
    "\n",
    "    # Returns obtained values\n",
    "    return freqs, power, xdata, ydata, amp, index, init, final\n",
    "\n",
    "\n",
    "# Define functions to perform data fit:\n",
    "def fitfunc(p, x):\n",
    "    return p[0] + p[1] * x\n",
    "\n",
    "\n",
    "def errfunc(p, x, y, err):\n",
    "    return (y - fitfunc(p, x)) / err\n",
    "\n",
    "\n",
    "# Define a function to compute a Power Law:\n",
    "def powerlaw(x, amp, index):\n",
    "    return amp * (x ** index)\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Compute 1D DFA for the time series\n",
    "# ---------------------------------------------------------------------\n",
    "def dfa1d(time_series, grau):\n",
    "    # Compute 1D DFA (adapted from Physionet), where the sclae frows according to 'Boxratio'. Returns the array\n",
    "    # 'vetoutput', where the first column is the logarithm of S scale and the second column is the logarithm of the\n",
    "    # fluctuation function\n",
    "\n",
    "    # 1. The time series {Xk} with k = 1, ..., N is integrated into the profile function Y(k)\n",
    "    x = np.mean(time_series)\n",
    "    time_series = time_series - x\n",
    "    yk = np.cumsum(time_series)\n",
    "    tam = len(time_series)\n",
    "\n",
    "    # 2. The (or profile) Y(k) is divided into N non-overlapping intervals of size S\n",
    "    sf = np.ceil(tam / 4).astype(np.int)\n",
    "    boxratio = np.power(2.0, 1.0 / 8.0)\n",
    "    vetoutput = np.zeros(shape=(1, 2))\n",
    "    s = 4\n",
    "\n",
    "    while s <= sf:\n",
    "        serie = yk\n",
    "        if np.mod(tam, s) != 0:\n",
    "            aux = s * int(np.trunc(tam / s))\n",
    "            serie = yk[0:aux]\n",
    "        t = np.arange(s, len(serie), s)\n",
    "        v = np.array(np.array_split(serie, t))\n",
    "        x = np.arange(1, s + 1)\n",
    "\n",
    "        # 3. Compute variance for each segment v = 1,…, n_s:\n",
    "        p = np.polynomial.polynomial.polyfit(x, v.T, grau)\n",
    "        yfit = np.polynomial.polynomial.polyval(x, p)\n",
    "        vetvar = np.var(v - yfit)\n",
    "\n",
    "        # 4. Compute the the fluctuation function of the DFA as the average of the variances in each interval\n",
    "        fs = np.sqrt(np.mean(vetvar))\n",
    "        vetoutput = np.vstack((vetoutput, [s, fs]))\n",
    "\n",
    "        # S scale grows in geometric series\n",
    "        s = np.ceil(s * boxratio).astype(np.int)\n",
    "\n",
    "    # Array with S scale log values and fluctuation function log values\n",
    "    vetoutput = np.log10(vetoutput[1::1, :])\n",
    "\n",
    "    # Split the columns of 'vetoutput'\n",
    "    x = vetoutput[:, 0]\n",
    "    y = vetoutput[:, 1]\n",
    "\n",
    "    # Linear Regression\n",
    "    slope, intercept, _, _, _ = stats.linregress(x, y)\n",
    "\n",
    "    # Compute line\n",
    "    predict_y = intercept + slope * x\n",
    "\n",
    "    # Compute error\n",
    "    pred_error = y - predict_y\n",
    "    # Returns the alpha value (slope), the 'vetoutput' vector, the X and Y vectors,\n",
    "    # a vector with line values, and the error vector\n",
    "    return slope, vetoutput, x, y, predict_y, pred_error\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Main section\n",
    "# ---------------------------------------------------------------------\n",
    "def main(data):\n",
    "    # Disble numpy errors and warnings\n",
    "    np.seterr(divide='ignore', invalid='ignore', over='ignore')\n",
    "\n",
    "    # -----------------------------------------------------------------\n",
    "    # General plot parameters:\n",
    "    # -----------------------------------------------------------------\n",
    "\n",
    "    # Define subplots\n",
    "    fig = plt.figure(figsize=(22,20),dpi=200)\n",
    "    fig.subplots_adjust(hspace=.3, wspace=.2)\n",
    "\n",
    "    # Font sizes:\n",
    "    size_font_axis_x = 10\n",
    "    size_font_axis_y = 10\n",
    "    size_font_title = 8\n",
    "    size_font_main = 15\n",
    "\n",
    "    # Main title\n",
    "    title_main = 'Time Series Spectral Analysis'\n",
    "\n",
    "    # -----------------------------------------------------------------\n",
    "    # Plot original series\n",
    "    # -----------------------------------------------------------------\n",
    "\n",
    "    # Define plot colors\n",
    "    cor_serie_original = 'r'\n",
    "    # Titles for axes on the original series plot\n",
    "    text_axis_x = 'Time'\n",
    "    text_axis_y = 'Amplitude'\n",
    "    text_title_original = 'Original Time Series Data'\n",
    "\n",
    "    # Plot original data series:\n",
    "    fig_handle = fig.add_subplot(2, 1, 1)\n",
    "    fig_handle.plot(data, '-', color=cor_serie_original)\n",
    "    fig_handle.set_title(text_title_original, fontsize=size_font_title)\n",
    "    fig_handle.set_xlabel(text_axis_x, fontsize=size_font_axis_x)\n",
    "    fig_handle.set_ylabel(text_axis_y, fontsize=size_font_axis_y)\n",
    "    fig_handle.ticklabel_format(style='sci', axis='x', scilimits=(0, 0))\n",
    "    fig_handle.grid()\n",
    "\n",
    "    # -----------------------------------------------------------------\n",
    "    # Compute and plot PSD\n",
    "    # -----------------------------------------------------------------\n",
    "\n",
    "    # Compute PSD\n",
    "    freqs, power, xdata, ydata, amp, index, init, fim = psd(data)\n",
    "\n",
    "    # Beta value is equivalent to the index:\n",
    "    beta = index\n",
    "\n",
    "    # Define plot colors:\n",
    "    cor_psd1 = 'k'\n",
    "    cor_psd2 = 'navy'\n",
    "\n",
    "    # PSD axes titles:\n",
    "    texto_psdx = 'Frequency (Hz)'\n",
    "    texto_psdy = 'Power'\n",
    "    texto_titulo_psd = r'Power Spectrum Density $\\beta$ = '\n",
    "\n",
    "    # -----------------------------------------------------------------\n",
    "    # Compute and plot DFA\n",
    "    # -----------------------------------------------------------------\n",
    "\n",
    "    # Compute 1D DFA\n",
    "    alfa, vetoutput, x, y, reta, error = dfa1d(data, 1)\n",
    "    # From Neelakshi et. al. (2019) \"Spectral fluctuation analysis of ionospheric inhomogeneities over Brazilian\n",
    "    # territory Part II: EF valley region plasma instabilities\"\n",
    "    # \"S. Heneghan and McDarby (2000) established an equivalence relation between the PSD exponent, b, and the DFA\n",
    "    # exponent, a, given by beta =  2 * alpha - 1. Kiyono (2015) showed that this relationship is valid for\n",
    "    # the higher order DFA subject to the constraint  0<a<m+1, where m is the order of detrending polynomial in the DFA\"\n",
    "    beta_theoretical = 2 * alfa - 1\n",
    "\n",
    "    # Plot PSD\n",
    "\n",
    "    fig_handle = fig.add_subplot(2, 2, 3)\n",
    "\n",
    "    fig_handle.plot(freqs, power, '-', color=cor_psd1, alpha=0.7)\n",
    "    fig_handle.plot(xdata, ydata, color=cor_psd2, alpha=0.8)\n",
    "    fig_handle.axvline(freqs[init], color=cor_psd2, linestyle='--')\n",
    "    fig_handle.axvline(freqs[-1], color=cor_psd2, linestyle='--')\n",
    "    fig_handle.plot(xdata, powerlaw(xdata, amp, index), 'r-', linewidth=1.5, label='$%.2f$' % beta)\n",
    "    fig_handle.set_xlabel(texto_psdx, fontsize=size_font_axis_x)\n",
    "    fig_handle.set_ylabel(texto_psdy, fontsize=size_font_axis_y)\n",
    "    fig_handle.set_title(texto_titulo_psd + r'%.2f (Theoretical $\\beta$ = 2$\\alpha$ +1 = %.2f)' % (beta,\n",
    "                                                                                                   beta_theoretical),\n",
    "                         loc='center', fontsize=size_font_title)\n",
    "    fig_handle.set_yscale('log')\n",
    "    fig_handle.set_xscale('log')\n",
    "    fig_handle.grid()\n",
    "\n",
    "    # Checks if DFA has valid value. If so, proceed with plot:\n",
    "\n",
    "    if not math.isnan(alfa):\n",
    "\n",
    "        # Define plot colors:\n",
    "        cor_dfa = 'darkmagenta'\n",
    "\n",
    "        # DFA axes title:\n",
    "        texto_dfax = '$log_{10}$ (s)'\n",
    "        texto_dfay = '$log_{10}$ F(s)'\n",
    "        texto_titulo_dfa = r'Detrended Fluctuation Analysis $\\alpha$ = '\n",
    "\n",
    "        # Plot DFA\n",
    "        fig_dfa = fig.add_subplot(2, 2, 4)\n",
    "        fig_dfa.plot(x, y, 's', color=cor_dfa, markersize=4, markeredgecolor='r', markerfacecolor='None', alpha=0.8)\n",
    "        fig_dfa.plot(x, reta, '-', color=cor_dfa, linewidth=1.5)\n",
    "        fig_dfa.set_title(texto_titulo_dfa + '%.4f' % alfa, loc='center', fontsize=size_font_title)\n",
    "        fig_dfa.set_xlabel(texto_dfax, fontsize=size_font_axis_x)\n",
    "        fig_dfa.set_ylabel(texto_dfay, fontsize=size_font_axis_y)\n",
    "        fig_dfa.grid()\n",
    "\n",
    "    else:\n",
    "        fig_dfa = fig.add_subplot(2, 2, 4)\n",
    "        fig_dfa.set_title('Detrended Fluctuation Analysis $\\alpha$ = ' + 'N.A.', loc='center',\n",
    "                          fontsize=size_font_title)\n",
    "        fig_dfa.grid()\n",
    "\n",
    "    # Draw and save figure (avoid showing to prevent blocking)\n",
    "    plt.suptitle(title_main, fontsize=size_font_main)\n",
    "    img_filename = 'ANALYSIS_PSD_DFA_2.png'\n",
    "    fig.set_size_inches(10, 5)\n",
    "    plt.savefig(img_filename, dpi=300, bbox_inches='tight', pad_inches=0.1)\n",
    "    plt.draw()\n",
    "\n",
    "    return alfa, beta_theoretical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Area_noise():\n",
    "    noise = np.array([[1.44769616, 1.82014602, 0.14640649, 0.66346162],\n",
    "                       [1.40956072, 1.1496771 , 2.73870097, 2.14094958],\n",
    "                       [1.19081834, 1.1496771 , 1.62616893, 2.14094958],\n",
    "                       [1.55327776, 1.40956072, 2.38220716, 2.73870097],\n",
    "                       [1.40997864, 1.44769616, 0.16779165, 0.14640649],\n",
    "                       [1.40997864, 1.19081834, 0.16779165, 1.62616893],\n",
    "                       [1.63758865, 1.82014602, 1.94768922, 0.66346162],\n",
    "                       [1.63758865, 1.55327776, 1.94768922, 2.38220716]])\n",
    "\n",
    "    Color_noise_pink = np.array([[1.02389761, 1.02804343, 1.23856761, 0.97903838],\n",
    "                                [0.96329599, 0.94043926, 1.23854284, 0.87054758],\n",
    "                                [0.96329599, 1.02389761, 1.23854284, 1.23856761],\n",
    "                                [0.95940765, 0.94043926, 0.82558347, 0.87054758],\n",
    "                                [0.95940765, 0.99140129, 0.82558347, 0.80742159],\n",
    "                                [1.01411267, 1.02804343, 0.86101667, 0.97903838],\n",
    "                                [1.01411267, 0.99140129, 0.86101667, 0.80742159]])\n",
    "\n",
    "    Color_noise_red = np.array([[1.49828585, 1.41541209, 2.40534815, 1.94596806],\n",
    "                               [1.49828585, 1.54609464, 2.40534815, 1.94690582],\n",
    "                               [1.47696749, 1.41541209, 1.70398688, 1.94596806],\n",
    "                               [1.47696749, 1.54609464, 1.70398688, 1.94690582]])\n",
    "\n",
    "    Color_noise_white = np.array([[ 0.52651906,  0.55626041,  0.42810822,  0.17993373],\n",
    "                                  [ 0.49084841,  0.46824137, -0.45950314, -0.30368259],\n",
    "                                  [ 0.47391403,  0.46824137,  0.21482022, -0.30368259],\n",
    "                                  [ 0.47391403,  0.52651906,  0.21482022,  0.42810822],\n",
    "                                  [ 0.52795351,  0.55626041, -0.12620915,  0.17993373],\n",
    "                                  [ 0.50716462,  0.49084841, -0.31563806, -0.45950314],\n",
    "                                  [ 0.50716462,  0.52795351, -0.31563806, -0.12620915]])\n",
    "\n",
    "    pm_noise = np.array([[0.78214647, 0.58558333, 0.08250894, 0.35462665],\n",
    "                           [0.95896388, 0.78214647, 0.38327095, 0.08250894],\n",
    "                           [0.88133414, 0.58558333, 1.12804452, 0.35462665],\n",
    "                           [0.88133414, 0.95559041, 1.12804452, 1.23922934],\n",
    "                           [0.98044892, 0.98755579, 1.17383784, 0.81952185],\n",
    "                           [0.98044892, 0.95559041, 1.17383784, 1.23922934],\n",
    "                           [0.98541874, 0.98755579, 0.74716073, 0.81952185],\n",
    "                           [0.98541874, 0.95896388, 0.74716073, 0.38327095]])\n",
    "\n",
    "    chaos_noise = np.array([[ 0.07471322,  0.02895051, -0.02780627, -1.7798434 ],\n",
    "                           [ 0.07471322,  0.46756863, -0.02780627, -0.08975267],\n",
    "                           [ 0.02908156,  0.02895051, -2.45732806, -1.7798434 ],\n",
    "                           [ 0.02908156,  0.28014732, -2.45732806, -1.59004539],\n",
    "                           [ 0.43151675,  0.46756863, -0.56256568, -0.08975267],\n",
    "                           [ 0.43151675,  0.28014732, -0.56256568, -1.59004539]])\n",
    "    \n",
    "    return noise,Color_noise_pink,Color_noise_red,Color_noise_white,pm_noise,chaos_noise"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
